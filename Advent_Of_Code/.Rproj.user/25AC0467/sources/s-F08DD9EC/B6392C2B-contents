---
title: "Coding project"
date: "Due Wednesday, 10th November 2021"
output: bookdown::pdf_document2 
header-includes:
- \usepackage{float}
- \floatplacement{figure}{H}
toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message= FALSE, warning = FALSE, fig.pos="H")
```

```{r warning=FALSE, message=FALSE}
# libraries
library(ggplot2)
library(cowplot)
library(psych)
library(dplyr)
library(caret) 
library(rpart.plot) 
library(FSelector)
library(arules)
library(arulesViz)
```

# Project part 1: EDA

## Introduction of the data set

The data set we are using in this project is the Titanic data set that is available on Kaggle. The data set has 12 variables describing 418 Titanic passengers. The first variable is the ID variable of passengers which we won't be using in this analysis. The variable `Survived` is a binary variable that takes 1 if the passenger has survived the disaster, and 0 otherwise. This would be the target variable in the analysis. The remaining 10 variables are the predictors we'll be using to predict if a passenger had survived or not. They include the `ticket class` that takes three values for first, second, and third class; the variable `Name` that is a character variable that include the name of the passengers; the categorical variable `Sex` that indicates the passenger's gender; the continuous variable `Age` indicates the age of the passenger; the numeric variable `SibSp` indicating the number of siblings or spouses the passenger has aboard; the numeric variable `Parch` indicating the number of parents and children aboard the Titanic; the character variable `Ticket` that takes the ticket number; the continuous variable `Fare` that indicates the passenger fare; the character variable `Cabin` indicating the cabin number; and finally the categorical variable `Embarked` indicates the port of embarkation.

Before we start exploring the data further, we eliminate the variables with missing values of the variables Age and Fare in order to have a data set with complete cases. The resulting data set has 331 observations.

```{r}
# import data
df <- read.csv("titanic-4erl4yab.csv")
```

```{r}
# remove observations with missing values
df <- df[complete.cases(df),]
```

## Summary statistics and distribution of the variables Age and Fare

The histogram of the variable `Age` in Figure 1 looks symmetric with one mode, and the mean value is close to the median. Also, the Q-Q plot in Figure 3 shows that the data points follow a straight line which are good indicators that the distribution is close to a normal distribution. The boxplot in Figure 2 of the variable shows that it has two outlier values. 

\begin{table}[!htbp] \centering 
  \caption{Summary statistics} 
  \label{} 
\begin{tabular}{@{\extracolsep{5pt}}lccccccc} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
Statistic & \multicolumn{1}{c}{Mean} & \multicolumn{1}{c}{Median} & \multicolumn{1}{c}{St. Dev.} & \multicolumn{1}{c}{Pctl(25)} & \multicolumn{1}{c}{Pctl(75)} & \multicolumn{1}{c}{Min} & \multicolumn{1}{c}{Max} \\ 
\hline \\[-1.8ex] 
Age & 30.181 & 27.000 & 14.105 & 21.000 & 39.000 & 0.170 & 76.000 \\ 
Fare & 40.982 & 16.000 & 61.229 & 8.050 & 40.633 & 0.000 & 512.329 \\ 
\hline \\[-1.8ex] 
\end{tabular} 
\end{table} 

The histogram of the variable `Fare` in Figure 1 looks right skewed as the right tail is much longer than the left one, with one mode, and the mean value is greater to the median. Also, the Q-Q plot in Figure 3 shows that the data points don't follow a straight line which indicates that the distribution is different from a normal distribution. The boxplot in Figure 2 of the variable shows that it has many outlier values. 

```{r warning=FALSE, message=FALSE, fig.cap="Histograms", fig.height=3}
#### Descriptive statistics
#stargazer::stargazer(df[,c("Age","Fare")],summary.stat = c("mean", "median", "sd","p25","p75", "min", "max"), title="Summary statistics")

# graphical displays
h1 <- ggplot(df, aes(Age, fill="#033769"))+
  geom_histogram(binwidth = 15)+
  ggtitle("Histogram of Age")+
  theme(legend.position = "none")
h2 <- ggplot(df, aes(Fare, fill="#033769")) +
  geom_histogram(binwidth = 50)+
  ggtitle("Histogram of Fare")+
  theme(legend.position = "none")
cowplot::plot_grid(h1, h2, ncol = 2, align = 'h')  
```


```{r warning=FALSE, message=FALSE, fig.cap="Boxplots", fig.height=3}
# graphical displays
b1 <- ggplot(df, aes(y=Age, fill="#033769"))+
  geom_boxplot()+
  ggtitle("Boxplot of Age")+
  theme(legend.position = "none")
b2 <- ggplot(df, aes(y=Fare, fill="#033769")) +
  geom_boxplot()+
  ggtitle("Boxplot of Fare")+
  theme(legend.position = "none")
cowplot::plot_grid(b1, b2, ncol = 2)  
```

```{r warning=FALSE, message=FALSE, fig.height=3, fig.cap="QQ plots"}
# graphical displays
q1 <- ggplot(df, aes(sample=Age))+
  stat_qq()+
  ggtitle("QQ plot of Age")
q2 <- ggplot(df, aes(sample=Fare)) +
  stat_qq()+
  ggtitle("QQ plot of Fare")
cowplot::plot_grid(q1, q2, ncol = 2)  
```

## Correlation matrix of the variables Age, Fare, and Pclass

The correlation matrix in Figure 4 shows that there is strong negative correlation between the variables `Fare` and `Pclass` which indicates that higher fares are associated with low classes.

The variable `Pclass` is negatively associated to the variable `Age` which indicates that older passengers tend to be in the high classes.

Finally, there is a weak positive correlation between the variables `Age` and `Fare`, indicating that older passengers tend to have higher fares.

```{r , fig.width=4, fig.height=4, fig.cap="Correlation matrix"}
pairs.panels(df[,c("Age","Fare","Pclass")], 
             method = "pearson", # correlation method
             hist.col = "#00AFBB",
             density = TRUE,  # show density plots
             ellipses = TRUE # show correlation ellipses
             )
```

## Frequency and proportion tables of the variables Sex and Survived

By examining the frequency table in Table 2 and 3 of the variable Sex we notice that there are 127 female passengers in this sample which is 38% of total passengers, and 204 male passengers, which is 62% of total passengers. Male passengers are dominant in this sample.

```{r}
# frequency table
table(df$Sex)%>%
    knitr::kable(caption = "Frequency table")

# relative frequency
round((table(df$Sex)/length(df$Sex)),2) %>%
    knitr::kable(caption = "Relative frequency table")
```

In order to assess the association between the target variable we are interested in predicting and the sex of passengers, we examine the two way table of the two variables in Table 4.

We notice that in this sample 100% of female passengers have survived, and 100% of male passengers didn't survive. In other words, the variable `Sex` is a perfect predictor of passengers survival. However, for the rest of the analysis, we will be interested in examining the predictive power of the other variables, so, we won't be including the variable `Sex` in the modeling parts. 

```{r}
# two way table
table(df$Survived, df$Sex) %>%
    knitr::kable(caption = "Two-way table of Sex and Survived")
```

## Association between the variable Age and Survived

In order to examine the association between the target variable `Survived` and the variable `Age`, we visualize the distribution of `Age` by the levels of the target variable.

The boxplots in Figure 5 show that the distribution of age is similar for both passengers who survived and those who did not, as the boxpots have the same level, indicating the same average (median) age, and the same size, indicating the same variance.

```{r warning=FALSE, message=FALSE, fig.cap="Distribution of passengers' age by survival", fig.height=3}
# side by side plot
ggplot(df, aes(y=Age, x=as.factor(Survived), fill="#033769"))+
  geom_boxplot()+
  ggtitle("Distribution of age by survival")+
  xlab("Survived")+
  theme(legend.position = "none")
```

The summary table of `Age` by `Survived` (Table 5) supports our previous observations as it shows that indeed the avergae (mean) age of passenger is the same for both cases of the target varable (mean=30), and the variance is also approximatly the same for both cases (Standard deviation of 13-15).

```{r}
# summary stats
group_by(df, Survived) %>%
  summarise(
    count = n(),
    mean = mean(Age, na.rm = TRUE),
    sd = sd(Age, na.rm = TRUE)
  )%>%knitr::kable(caption = "Summary of Age by Survived")
```

To examine the significance of our conclusion, we use a two-sample t-test to test the null hypothesis that the there is no difference in the mean of the variable Age for the levels of the variable Survived. 

The p-value of the t-test (Table 6) is greater than the significance level $\alpha=0.05$, which indicates that we can't reject the null hypothesis of the test, and we conclude that there is no difference in mean of age of passengers who survived and those who did not. 

```{r}
# two(sample t.test 
t.test(Age ~ Survived, df) %>%
  broom::tidy()%>%
  knitr::kable(caption="Summary of the two sample t-test for mean of Age")
```

## Association between the variable Fare and Survived

In order to understand the association between the target variable `Survived` and the variable `Fare`, we examine graphically the distribution of `Fare` by the levels of the target variable.

The boxplots in Figure 6 show that Fare has a higher median and higher variance for passengers who survived, as the boxplot of passenegrs who survived is higher in level and larger in size than the boxplot of Fare for passengers who did not survive.

```{r warning=FALSE, message=FALSE, fig.cap="Distribution of passengers' fare by survival", fig.height=3}
# side by side plot
ggplot(df, aes(y=Fare, x=as.factor(Survived), fill="#033769"))+
  geom_boxplot()+
  ggtitle("Distribution of passenger fare by survival")+
  xlab("Survived")+
  theme(legend.position = "none")
```

The summary table of `Fare` by `Survived` in Table 7 shows that indeed the avergae (mean) fare is higher for the passengers who survived, and the Standard deviation as a measure of spread is also higher for passengers who survived.

```{r}
# summary stats
group_by(df, Survived) %>%
  summarise(
    count = n(),
    mean = mean(Fare, na.rm = TRUE),
    sd = sd(Fare, na.rm = TRUE)
  )%>%knitr::kable(caption = "Summary of Fare by Survived")
```

To examine the significance of our conclusions, we use a two-sample t-test to test the null hypothesis that the there is no difference in the mean of the variable `Fare` for the levels of the variable `Survived`. 

The p-value of the t-test (see Table 8) $p-value=0.001781$ is less than the significance level $\alpha=0.05$, which indicates that we reject the null hypothesis, and we conclude that there is a significant difference in mean of fare of passengers who survived and those who did not. 

```{r}
# two(sample t.test 
t.test(Fare ~ Survived, df)%>%
  broom::tidy()%>%
  knitr::kable(caption="Summary of the two sample t-test for mean of Fare")
```

## Conclusion

We conclude from the EDA that the variable `Sex` is a perfect predictor of the target variable. The variable `Age` doesn't vary significantly by levels of the target variable which means that it is not a good predictor. Finally, the variable `Fare` varies significantly by levels of the target variable, which indicates that it could be a good predictor.

# Project part 2: Classification

## Fit a simple decision tree to predict the variable Survived

Before we fit the decision tree, we split the data to a train set of 80% of data which we'll use to train the model, and a test set of 20% of data which we'll use to test the accuracy of predictions of the model.

The resulting tree shown in Figure 7 uses the variables Parch, Fare, SibSp, Age and Embarked to classify passengers who survived or did not survive the Titanic.  

```{r fig.width=9, fig.cap="Decision tree"}
# split to train and test
# set the seed
set.seed(77)
# split data to train and test: 80% train and 20% test
smp_size <- floor(0.8 * nrow(df))
train_ind <- sample(seq_len(nrow(df)), size = smp_size)
train <- df[train_ind, ]
test <- df[-train_ind, ]

# Classification tree
# train model
fit.tree <- rpart(Survived ~ ., data=train[complete.cases(train),-c(1,4,9,5,11)], method = 'class')
rpart.plot(fit.tree)
```

The confusion matrix in Table 9 of the decision tree indicates the number of correctly predicted cases and the cases that were wrongly predicted. The columns show the actual values, and the rows show the predictions. We notice that the model correctly predicts 132 death cases and 69 survival cases. It mistakenly classifies 26 dead passengers as survived, and 37 passengers who survived as dead.

```{r}
## prediction
pred.tree <- predict(fit.tree, train[complete.cases(train),-c(1,4,9,5,11)], type = 'class')
table(pred.tree, as.factor(train$Survived[complete.cases(train)])) %>%
    knitr::kable(caption="Confusion matrix of the model on the train set")
```

The overall accuracy of the model is 76%, with a sensitivity of 83% and a specificity of 65%. This indicates that the model has a good ability to identify passengers who did not survived (sensitivity), however it has a lower ability to identify passengers who survived (specificity).  

```{r}
# Confusion matrix
cfmtrx_ctree <- confusionMatrix(pred.tree,reference=as.factor(train$Survived[complete.cases(train)]), mode="everything")

cfmtrx_ctree
```

## Cross Validation

To validate the decision tree, we use the CP parameter (Complexity Parameter) error cross validation. This method helps to estimate how accurately the model performs. 

To do so, we use the least cross validated error to prune the tree we fit previously to avoid any overfitting of the data. Practically, we select the cp having the least cross-validated error to use it to prune the tree. 

The resulting tree (Figure 8) is smaller than the previous one, and uses as predictors the variables Parch, Fare, SibSp, Age and Embarked to classify passengers.


```{r fig.width=8, fig.cap="Pruned tree"}
# cv
ptree<- prune(fit.tree, cp= fit.tree$cptable[which.min(fit.tree$cptable[,"xerror"]),"CP"])
rpart.plot(ptree)
```

To evaluate the pruned tree model, we use it to make predictions on the test data. The confusion matrix of the test set (table 11)shows that the model correctly classifies 34 passengers as dead out of 46, and correctly classifies 6 passengers as survived out of 21. This shows that this model doesn't perform on the test set as well as it performs on the train set (table 10) specially in classifying passengers as survived.

```{r}
## prediction on train set
pred.tree.test <- predict(ptree, train[complete.cases(test),-c(1,4,9,5,11)], type = 'class')
table(pred.tree.test, as.factor(train$Survived[complete.cases(train)])) %>%
    knitr::kable(caption="Confusion matrix of the pruned tree on the train set")

## prediction on test set
pred.tree.test <- predict(ptree, test[complete.cases(test),-c(1,4,9,5,11)], type = 'class')
table(pred.tree.test, as.factor(test$Survived[complete.cases(test)])) %>%
    knitr::kable(caption="Confusion matrix of the pruned tree on the test set")
```

The overall accuracy of the model on the test set is 59%, with a sensitivity of 73% and a specificity of only 28%.

```{r}
# Confusion matrix
cfmtrx_ctree.tst <- confusionMatrix(pred.tree.test,reference=as.factor(test$Survived[complete.cases(test)]), mode="everything")

cfmtrx_ctree.tst
```

Figure 9 illustrates variables importance in the pruned tree. The most important variable is Fare, followed by Parch, and then Embarked.

```{r fig.height=3, fig.cap="Variable importance in the pruned tree"}
# variables importance
barplot(t(ptree$variable.importance),horiz=FALSE)
```

## Feature selection using Gain Ratio

We can use another technic to find the importance of variables, that is the Gain Ratio.

Figure 10 shows that the most important feature is Embarked, which is the third most important variable in the tree model. We notice that according to the Gain Ratio, the variables used in the tree aren't the most important in the data set. 

```{r fig.height=4, fig.cap="Features importance with the gain ratio"}
# gain ratio
dfcol <- df[,-c(1,4,9,5,11)]           # remove not used variables
weights.gr <- gain.ratio(Survived~.,dfcol , unit ="log2") # default log e
# plot
weights.gr$var <- rownames(weights.gr)
ggplot(weights.gr, aes(y=attr_importance, x=reorder(var,attr_importance)))+
  geom_col()+
  coord_flip()+
  xlab("Features")+ylab("Importance")+
  ggtitle("Features importance with the gain ratio")
```

# Project part 3: Association

## Examine the association data

```{r}
# convert variables to logical of factor
dfcol[, c(1,2,4,5,7)] <- lapply(dfcol[,c(1,2,4,5,7)], as.factor)
dfcol$Age <- cut.default(dfcol$Age, 5) # split age to 5 levels factor
dfcol$Fare <- cut.default(dfcol$Fare, 5) # split Fare to 5 levels factor
# create transactions
tData <- as(dfcol[], "transactions")
```

By inspecting the associations in the transactions data, we can see for example in the first association, that a passenger who did not survive the disaster was in 3rd class, aged between 30 and 45 years, and had no siblings, parents, spouse or children on board, had a fare ranging between -0.512 and 102, and embarked through Queenstown Port.

```{r}
# inspect association data
inspect(head(tData, 3))
```

Bellow is a summary of the transaction data:

```{r}
# summarize
summary(tData)
```

To visualize the association, we can examine the frequency of items in the association data. Figure 11 illustrates items frequency in the data. It shows the most frequent item in each variable. We notice that the most frequent fare is between -0.512 and 102, and most passengers don't have any parents, children, sibling or spouse on board, Embarked from Southampton port, did not survive, aged between 15 and 30 years and from third class. 


```{r fig.cap="Tansactions' items frequency plot", fig.height=3}
# visualize
itemFrequencyPlot(tData, topN=10, type="absolute", main="Item Frequency") # plot frequent items
```

## Association rules with Apriori: Examine the rules

```{r results='hide'}
# creating rules
titanic_rules <- apriori(tData, parameter = list(support = 0.01, confidence = 0.7,maxlen=3))
```

The top rules according to confidence (=1) indicate that passengers who had 3 siblings or parents on board have also embarked from Southampton Port, and those who had 4 siblings or parents on board belong to the third class and have a fare between -0.512 and 102.

```{r results='hide'}
# 'high-confidence' rules.
rules_conf <- sort (titanic_rules, by="confidence", decreasing=TRUE) 
# show the support, lift and confidence 
inspect(head(rules_conf))
```

The top rules according to lift (=5.201) indicate that passengers who are aged less than 15 years and who have one sibling or parent on board also have a spouse or child on board.

```{r}
# 'high-lift' rules.
rules_lift <- sort (titanic_rules, by="lift", decreasing=TRUE) 
# show the support, lift and confidence 
inspect(head(rules_lift))
```

Bellow is a summary of the rules:

```{r}
# Summarize
summary(titanic_rules)
```

Figure 12 illustrates a scatterplot of the confidence, support and lift of the rules generated. This scatter plot shows that the confidence between 0.75 and 0.8, and support between 0 and 0.1, maximize the lift of the rule. We also notice that the higher the lift the lower the support.

```{r fig.height=3, fig.cap="Scatterplot for 404 rules"}
# Visualize
plot(titanic_rules)
```

Another way to visualize the rules is to use a grouped matrix bubbled chart (Figure 13) that illustrates the support and lift measures by the size and color of the circles, respectively. We notice that rule 14 has the highest support, and rules 4 has the highest lift for Parch=1.

```{r fig.height=10, fig.cap="Grouped matrix for the 404 rules"}
# Visualize
plot(titanic_rules, method="grouped")
```

Finally, we can use a network diagram to visualize the top 10 rules of passengers who survived (Figure 14). 

We can see in the plot that passengers who survived are aged between 15 and 30, have no children/parents and no siblings/spouse on board and are from 2nd class.

```{r fig.height=4, fig.cap="Graph for the top 10 rules of passengers who survived"}
# subset rules 
titanic_rules_survived <- subset(titanic_rules,subset=rhs %pin% "Survived=1")
# Visualize
plot(titanic_rules_survived,method="graph")
```

\pagebreak

# Project part 4: Regression

## Fit a simple regression model

We fit a simple regression model to predict the `Survived` variable using the variable `Pclass`.
The estimated regression equation by the model is :

$$Survived = 0.52901 -0.06784 \times Pclass$$

The estimated coefficient of `Pclass` is statistically significant to the model since the p-value of the t-test of the significance of the coefficient is less than $\alpha=0.05$, which means that the coefficient of `Pclass`is statistically different from 0.

The estimated coefficient of `Pclass` is negative indicating that there is a negative association between `Survived` and `Pclass`, which means that the lower the value of `Pclass` is (meaning the higher the class since first class is 1 and third class is 3), the more likely the passenger to Survive.

The R squared of the model is 0.0139, which means that this model can explain 1.39% of variance in the dependent variable.

```{r}
# fit linear model
reg1 <- lm(Survived ~ Pclass, data=df)
# model summary
```

\begin{table}[!htbp] \centering 
  \caption{Summayr of the simple regression model} 
  \label{} 
\begin{tabular}{@{\extracolsep{5pt}}lc} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{1}{c}{\textit{Dependent variable:}} \\ 
\cline{2-2} 
\\[-1.8ex] & Survived \\ 
\hline \\[-1.8ex] 
 Pclass & $-$0.068$^{**}$ \\ 
  & (0.032) \\ 
  & \\ 
 Constant & 0.529$^{***}$ \\ 
  & (0.073) \\ 
  & \\ 
\hline \\[-1.8ex] 
Observations & 331 \\ 
R$^{2}$ & 0.014 \\ 
Adjusted R$^{2}$ & 0.011 \\ 
Residual Std. Error & 0.484 (df = 329) \\ 
F Statistic & 4.637$^{**}$ (df = 1; 329) \\ 
\hline 
\hline \\[-1.8ex] 
\textit{Note:}  & \multicolumn{1}{r}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\ 
\end{tabular} 
\end{table}

The scatterplot of survived against Pclass shows that the data points can be modeled with a regression line with a negative slope, like we found in the regression model. However, the data points don't seem to have a linear relationship since the are not continuous.

```{r fig.height=3, fig.cap="Scatterplot of Survived against Pclass with a fitted regression line"}
# scatter plot
ggplot(df, aes(x=Pclass, y=Survived))+
  geom_point()+
  geom_smooth(method="lm")
```


## Fit a multiple regression model

We fit a multiple regression model to predict the `Survived` variable using the variables `Pclass`, `Age` and `Fare`.
The estimated regression equation by the model is :

$$Survived = 0.4652261 -0.0267910 \times Pclass -0.0028925 \times Age +0.0015408 \times Fare$$

The estimated coefficient of `Pclass` is no longer statistically significant to the model since the p-value of the t-test of the significance of the coefficient is greater than $\alpha=0.05$, which means that the coefficient of `Pclass`is not statistically different from 0.

The estimated coefficient of `Age` is also not statistically significant to the model since the p-value of the t-test of the significance of the coefficient is greater than $\alpha=0.05$, which means that the coefficient of `Age`is not statistically different from 0.

The estimated coefficient of `Fare` is statistically significant to the model since the p-value of the t-test of the significance of the coefficient is less than $\alpha=0.05$, which means that the coefficient of `Fare`is statistically different from 0.

The estimated coefficient of `Fare` is positive indicating that there is a positive association between `Fare` and `Survived`, which means that the higher the value of `Fare` is, the more likely the passenger to Survive.

The R squared of the model is 0.0423, which means that this model can explain 4.23% of variance in the dependent variable.

```{r}
# fit linear model
reg2 <- lm(Survived ~ Pclass+Age+Fare, data=df)
# model summary
```

\begin{table}[!htbp] \centering 
  \caption{Summary of the multiple regression model} 
  \label{} 
\begin{tabular}{@{\extracolsep{5pt}}lc} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{1}{c}{\textit{Dependent variable:}} \\ 
\cline{2-2} 
\\[-1.8ex] & Survived \\ 
\hline \\[-1.8ex] 
 Pclass & $-$0.027 \\ 
  & (0.042) \\ 
  & \\ 
 Age & $-$0.003 \\ 
  & (0.002) \\ 
  & \\ 
 Fare & 0.002$^{***}$ \\ 
  & (0.001) \\ 
  & \\ 
 Constant & 0.465$^{***}$ \\ 
  & (0.142) \\ 
  & \\ 
\hline \\[-1.8ex] 
Observations & 331 \\ 
R$^{2}$ & 0.042 \\ 
Adjusted R$^{2}$ & 0.034 \\ 
Residual Std. Error & 0.479 (df = 327) \\ 
F Statistic & 4.824$^{***}$ (df = 3; 327) \\ 
\hline 
\hline \\[-1.8ex] 
\textit{Note:}  & \multicolumn{1}{r}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\ 
\end{tabular} 
\end{table} 

The scatterplot of survived against Fare shows that the data points can be modeled with a regression line with a positive slope, like we found in the regression model. However, there seems that there is an outlier value which affects the regression line to be have a positive slop which could be affecting our conclusions and the results of the model.

```{r fig.height=3, fig.cap="Scatterplot of Survived against Fare with a fitted regression line"}
# scatter plot
ggplot(df, aes(x=Fare, y=Survived))+
  geom_point()+
  geom_smooth(method="lm")
```

The scatterplot of survived against Age shows that the data points are modeled with a horizontal line, which indicates that there is no linear association between Age and survival of passengers.

```{r fig.height=3, fig.cap="Scatterplot of Survived against age with a fitted regression line"}
# scatter plot
ggplot(df, aes(x=Age, y=Survived))+
  geom_point()+
  geom_smooth(method="lm")
```

## Fit a multiple regression model with interaction

We fit a multiple regression model to predict the `Survived` variable using the variables `Pclass`, `Age`, `Fare` and an interaction term between `Age` and `Pclass` to see if the interaction between these two variables could be significant to the model.
The estimated regression equation by the model is :

$$Survived = 0.344 +0.0321 \times Pclass +9,2e-4 \times Age +0.0014 \times Fare -2,025e-3 \times Age:Pclass$$

The estimated coefficients of `Pclass` and `Age` in this model are still not statistically significant to the model since the p-values of the t-test of the significance of their coefficients are greater than $\alpha=0.05$, which means that the coefficients are not statistically different from 0.

The estimated coefficient of `Fare` is still statistically significant to the model.

The estimated coefficient of the interaction between `Age` and `Pclass` is not statistically significant to the model since the p-value of the t-test of the significance of the coefficient is greater than $\alpha=0.05$, which means that the coefficient of the interaction term is not statistically different from 0.

The estimated coefficient of `Fare` is positive indicating that there is a positive association between `Fare` and `Survived`, which means that the higher the value of `Fare` is, the more likely the passenger to Survive.

The R squared of the model is 0.0443, which means that this model can explain 4.43% of variance in the dependent variable.

```{r}
# fit linear model
reg3 <- lm(Survived ~ Pclass*Age+Fare, data=df)
# model summary
```

\begin{table}[!htbp] \centering 
  \caption{Summary of the regression model with interaction term} 
  \label{} 
\begin{tabular}{@{\extracolsep{5pt}}lc} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{1}{c}{\textit{Dependent variable:}} \\ 
\cline{2-2} 
\\[-1.8ex] & Survived \\ 
\hline \\[-1.8ex] 
 Pclass & 0.033 \\ 
  & (0.085) \\ 
  & \\ 
 Age & 0.001 \\ 
  & (0.005) \\ 
  & \\ 
 Fare & 0.001$^{***}$ \\ 
  & (0.001) \\ 
  & \\ 
 Pclass:Age & $-$0.002 \\ 
  & (0.003) \\ 
  & \\ 
 Constant & 0.344$^{*}$ \\ 
  & (0.206) \\ 
  & \\ 
\hline \\[-1.8ex] 
Observations & 331 \\ 
R$^{2}$ & 0.044 \\ 
Adjusted R$^{2}$ & 0.033 \\ 
Residual Std. Error & 0.479 (df = 326) \\ 
F Statistic & 3.778$^{***}$ (df = 4; 326) \\ 
\hline 
\hline \\[-1.8ex] 
\textit{Note:}  & \multicolumn{1}{r}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\ 
\end{tabular} 
\end{table} 

The scatterplot of survived against he interaction between `Age` and `Pclass` shows that the data points are modeled with a regression line with a negative slope, which indicates that old passengers from lower classes are not likely to survive, and young passengers from high classes are likely to survive.

```{r fig.height=3, fig.cap="Scatterplot of Survived against the interaction between Age and Pclass with a fitted regression line"}
# scatter plot
ggplot(df, aes(x=Pclass*Age, y=Survived))+
  geom_point()+
  geom_smooth(method="lm")
```















